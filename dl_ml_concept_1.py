# -*- coding: utf-8 -*-
"""DL-ML-Concept-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H6fjiAgJv3ogwjzDwMBVsG6nPGkCUPDP
"""

!pip install nltk

"""# Tokenization"""

corpus = "Hi I am vaibhav's.I Love coding! ,I am exploring AI everyday."

corpus

import nltk
nltk.download('punkt')

#Tokinize
# paragraph --> Sentence
from nltk.tokenize import sent_tokenize
doc1 = sent_tokenize(corpus)
print(doc1)

#Tokinize
# Sentence into words
from nltk.tokenize import word_tokenize
doc2 = word_tokenize(corpus)
print(doc2)

#Tokinize
for sentence in doc1:
  print(word_tokenize(sentence))

#Tokinize
# paragraph --> words
# sentence  --> words
# wordpunct_tokenize --> it splited each and every words like -> ` , '
from nltk.tokenize import wordpunct_tokenize
for sentence in doc1:
  print(wordpunct_tokenize(sentence))

"""# Stemming And Its Types -  Text Preprocessing

**Stemming** : Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.Stemming is important in natural language understanding(NLU) and Natural Language Processing(NLP).

Usecase : Stemming use for email spam or Ham
"""

## Classification Problem
## Comments of product is a positive review or negative review
## Reviews ----> eating, eat, eaten   [going,gone,goes]
words=["eating","eats","eaten","writing","writes","programming","programs","history","finally","finalized"]

"""##PorterStemmer"""

from nltk.stem import PorterStemmer
stemming = PorterStemmer()
for word in words:
  print(word + "---->"+stemming.stem(word))

stemming.stem('congratulations')

stemming.stem('sitting')

"""**RegexpStemmer class** : NLTK has RegexStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression.Let us see an example."""

from nltk.stem import RegexpStemmer

reg_stemmer = RegexpStemmer('ing$|s$|e$|able$',min=4)

reg_stemmer.stem('eating')

reg_stemmer.stem('ingeating')



"""# Snowball Stemmer"""

from nltk.stem import SnowballStemmer

snowballstemmer = SnowballStemmer('english')

for word in words:
  print(word+ "----->"+snowballstemmer.stem(word))

stemming.stem("fairly"),stemming.stem("sportingly") # It give result but not as expected so snowballstemmer comes into in picture

snowballstemmer.stem("fairly"),snowballstemmer.stem("sportingly") # Snowballstemmer give good output as compare to PorterStemmer

snowballstemmer.stem('goes')

stemming.stem("goes")

"""# Wordnet Lemmatizer
Lemmatization techinque is like stemming. The output we will get after lemmatization is call 'lemma', Which is a root word rather than root stem,the output of stemming. After lemmatization,we will be geeting a valid word that means the same thing.

NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus.This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand ot with an example -
"""

## Q & A ,CHATBOTS,TEXT SUMMARZIZATION
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

'''
POS- Noun-n
verb-v
adjective-a
adverb-r
'''
# below we are passing pos='n',pos='v',pos='a',pos='r'
lemmatizer.lemmatize("going",pos='n')

words=["eating","eats","eaten","writing","writes","programming","programs","history","finally","finalized"]

for word in words:
  print(word + "---->"+lemmatizer.lemmatize(word,pos='v')) # change pos=  and see changes

lemmatizer.lemmatize('goes',pos='v')

lemmatizer.lemmatize('fairly',pos='v'),lemmatizer.lemmatize('sportingly')



"""# Stop Words"""

## Speech Of DR APJ Abdul Kalam
paragraph = """I have three visions for India. In 3000 years of our history, people from all over
               the world have come and invaded us, captured our lands, conquered our minds.
               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,
               the French, the Dutch, all of them came and looted us, took over what was ours.
               Yet we have not done this to any other nation. We have not conquered anyone.
               We have not grabbed their land, their culture,
               their history and tried to enforce our way of life on them.
               Why? Because we respect the freedom of others.That is why my
               first vision is that of freedom. I believe that India got its first vision of
               this in 1857, when we started the War of Independence. It is this freedom that
               we must protect and nurture and build on. If we are not free, no one will respect us.
               My second vision for India’s development. For fifty years we have been a developing nation.
               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world
               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.
               Our achievements are being globally recognised today. Yet we lack the self-confidence to
               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?
               I have a third vision. India must stand up to the world. Because I believe that unless India
               stands up to the world, no one will respect us. Only strength respects strength. We must be
               strong not only as a military power but also as an economic power. Both must go hand-in-hand.
               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of
               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.
               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.
               I see four milestones in my career"""

from nltk.stem import PorterStemmer

from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')

stopwords.words('english')

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

sentences = nltk.sent_tokenize(paragraph)

type(sentences)

## Apply Stopwords And Filter And then Apply Stemming

for i in range(len(sentences)):
  words = nltk.word_tokenize(sentences[i])
  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # remove all stopword
  sentences[i] = ' '.join(words) # converting all the list of words into sentences

sentences

## Apply SnowballStemmer
from nltk.stem import SnowballStemmer
snowballstemmer = SnowballStemmer('english')
sentences = nltk.sent_tokenize(paragraph)
## Apply Stopwords And Filter And then Apply SnowballStemmer

for i in range(len(sentences)):
  words = nltk.word_tokenize(sentences[i])
  words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # remove all stopword
  sentences[i] = ' '.join(words) # converting all the list of words into sentences

sentences

## Apply WordNetLemmatizer
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
sentences = nltk.sent_tokenize(paragraph)
for i in range(len(sentences)):
  words = nltk.word_tokenize(sentences[i])
  words = [lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))] # remove all stopword
  sentences[i] = ' '.join(words) # converting all the list of words into sentences

sentences



"""# Parts Of Speech Tags

"Taj Mahal is a beautiful Monument"

CC coordinating conjunction
CD cardinal digit
DT determiner
EX existential there (like: “there is” … think of it like “there exists”)
FW foreign word
IN preposition/subordinating conjunction
JJ adjective – ‘big’
JJR adjective, comparative – ‘bigger’
JJS adjective, superlative – ‘biggest’
LS list marker 1)
MD modal – could, will
NN noun, singular ‘- desk’
NNS noun plural – ‘desks’
NNP proper noun, singular – ‘Harrison’
NNPS proper noun, plural – ‘Americans’
PDT predeterminer – ‘all the kids’
POS possessive ending parent’s
PRP personal pronoun –  I, he, she
PRP$ possessive pronoun – my, his, hers
RB adverb – very, silently,
RBR adverb, comparative – better
RBS adverb, superlative – best
RP particle – give up
TO – to go ‘to’ the store.
UH interjection – errrrrrrrm
VB verb, base form – take
VBD verb, past tense – took
VBG verb, gerund/present participle – taking
VBN verb, past participle – taken
VBP verb, sing. present, non-3d – take
VBZ verb, 3rd person sing. present – takes
WDT wh-determiner – which
WP wh-pronoun – who, what
WP$ possessive wh-pronoun, eg- whose
WRB wh-adverb, eg- where, when
"""

## Speech Of DR APJ Abdul Kalam
paragraph = """I have three visions for India. In 3000 years of our history, people from all over
               the world have come and invaded us, captured our lands, conquered our minds.
               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,
               the French, the Dutch, all of them came and looted us, took over what was ours.
               Yet we have not done this to any other nation. We have not conquered anyone.
               We have not grabbed their land, their culture,
               their history and tried to enforce our way of life on them.
               Why? Because we respect the freedom of others.That is why my
               first vision is that of freedom. I believe that India got its first vision of
               this in 1857, when we started the War of Independence. It is this freedom that
               we must protect and nurture and build on. If we are not free, no one will respect us.
               My second vision for India’s development. For fifty years we have been a developing nation.
               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world
               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.
               Our achievements are being globally recognised today. Yet we lack the self-confidence to
               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?
               I have a third vision. India must stand up to the world. Because I believe that unless India
               stands up to the world, no one will respect us. Only strength respects strength. We must be
               strong not only as a military power but also as an economic power. Both must go hand-in-hand.
               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of
               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.
               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.
               I see four milestones in my career"""

from nltk.corpus import stopwords
sentences = nltk.sent_tokenize(paragraph)

sentences

nltk.download('averaged_perceptron_tagger')

## We will find the Pos Tag
for i in range(len(sentences)):
  words = nltk.word_tokenize(sentences[i])
  words = [word for word in words if word not in set(stopwords.words('english'))]
  pos_tag = nltk.pos_tag(words)
  print(pos_tag)

for word in "Taj Mahal is a beautiful Monument".split():
  words = nltk.word_tokenize(word)
  print(nltk.pos_tag(words))


print("=======================================================")
print(nltk.pos_tag("Taj Mahal is a beautiful Monument".split()))

"""# Named Entity Recogination"""

sentence="The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures."
"""
Person Eg: Krish C Naik
Place Or Location Eg: India
Date Eg: September,24-09-1989
Time  Eg: 4:30pm
Money Eg: 1 million dollar
Organization Eg: iNeuron Private Limited
Percent Eg: 20%, twenty percent
"""

words = nltk.word_tokenize(sentence)

tag_elements = nltk.pos_tag(words)

nltk.download('maxent_ne_chunker')

nltk.download('words')

!pip install svgling

nltk.ne_chunk(tag_elements)



"""# One Hot Encoding

**Bag Of Word**
"""

# Download SMSSpamCollection file  from :  https://archive.ics.uci.edu/dataset/228/sms+spam+collection
import pandas as pd
messages=pd.read_csv('/content/SMSSpamCollection',
                    sep='\t',names=["label","message"],encoding='latin1')

messages

## Data cleaning and Preprocessing
import re
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

corpus=[]
for i in range(0,len(messages)):
    review=re.sub('[^a-zA-z]',' ',messages['message'][i])
    review=review.lower()
    review=review.split()
    review=[ps.stem(word) for word in review if not word in stopwords.words('english')]
    review=' '.join(review)
    corpus.append(review)

corpus

"""# Create Bag of Words"""

## Create the Bag OF Words model
from sklearn.feature_extraction.text import CountVectorizer
## for Binary BOW enable binary=True
cv=CountVectorizer(max_features=100,binary=True)

X= cv.fit_transform(corpus).toarray()

X.shape

X

"""# N-Grams"""

cv.vocabulary_

## Create the Bag OF Words model with NGram
from sklearn.feature_extraction.text import CountVectorizer
## for Binary BOW enable binary=True
cv=CountVectorizer(max_features=100,binary=True,ngram_range=(2,3)) # you can change ngram_range=(1,1) , (1,2) , (2,1) , (2,2)
X= cv.fit_transform(corpus).toarray()

cv.vocabulary_

X

"""**TF-IDF (Term Frequency - Inverse Document Frequency)**

TF = No. of rep of words in sentence / No. of Words in sentence

IDF = loge(No. of sentences / No. of sentences containing the word)
"""

# Download SMSSpamCollection file  from :  https://archive.ics.uci.edu/dataset/228/sms+spam+collection
import pandas as pd
messages=pd.read_csv('/content/SMSSpamCollection',
                    sep='\t',names=["label","message"],encoding='latin1')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
wordlemmatize = WordNetLemmatizer()

corpus=[]
for i in range(0,len(messages)):
    review=re.sub('[^a-zA-z]',' ',messages['message'][i])
    review=review.lower()
    review=review.split()
    review=[wordlemmatize.lemmatize(word) for word in review if not word in stopwords.words('english')]
    review=' '.join(review)
    corpus.append(review)

"""**Create TF-IDF And NGrams**"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer(max_features=100)
X= tfidf.fit_transform(corpus).toarray()

import numpy as np
np.set_printoptions(edgeitems=30, linewidth=100000,formatter=dict(float=lambda X: "%.3g" % X))

X

"""**N-Grams**"""

tfidf = TfidfVectorizer(max_features = 100, ngram_range=(2,2))
X= tfidf.fit_transform(corpus).toarray()

tfidf.vocabulary_

X

"""**Word Embeddings :**


In natural language processing (NLP), word embedding is a tearm used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.

Word Embedding type:

1. count of frequency

  a. OHE
  B. BOW
  C. TF-IDF

2.Deep learning Trained Model

  a. word2vec technique  --> CBOW , skipgram
"""



"""**Word2Vec**

Word2vec is a technique for NLP published in 2013.The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.Once trained,such a model can detect synonymous words or suggest additional words for a partial sentences.As the name implies,word2vec represents each distinct word with a particular list of numbers called a vector.


**Word2vec**  --> Pretrained Model, Train Model from Scratch

CBOW, Skipgram




"""



"""**Word2Vec Implementation**"""

!pip install gensim

import gensim

from gensim.models import Word2Vec, KeyedVectors

## Reference  : https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin

import gensim.downloader as api
wv = api.load('word2vec-google-news-300')
vec_king = wv['king']

vec_king

vec_king.shape

wv['cricket']

wv.most_similar('cricket')

wv.most_similar('happy')

wv.most_similar('hockey','sports')

vec = wv['king'] - wv['man'] + wv['woman']

vec

wv.most_similar([vec])